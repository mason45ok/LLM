{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.35.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mason\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''What is Machine Learning?'''\n",
    "\n",
    "paragraph = ''' Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance \n",
    "                on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or \n",
    "                decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection \n",
    "                of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning \n",
    "                is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, \n",
    "                theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory \n",
    "                data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics. '''\n",
    "            \n",
    "encoding = tokenizer.encode_plus(text=question,text_pair=paragraph)\n",
    "\n",
    "inputs = encoding['input_ids']  #Token embeddings\n",
    "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
    "start_scores, end_scores = model(torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "print(type(start_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_logits\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "print(type(start_scores))\n",
    "\n",
    "# 如果它不是 torch.Tensor，嘗試將其轉換為 torch.Tensor\n",
    "# if not isinstance(start_scores, torch.Tensor):\n",
    "#     start_scores = torch.tensor(start_scores)\n",
    "\n",
    "# # 再次檢查 start_scores 的類型\n",
    "# print(type(start_scores))\n",
    "\n",
    "# # 最後檢查數據類型\n",
    "# print(start_scores.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mason\\Desktop\\作業\\科技系\\LLM\\Fine-tuned.ipynb 儲存格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/Fine-tuned.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m start_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49margmax(start_scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/Fine-tuned.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m end_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(end_scores)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/Fine-tuned.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(tokens[start_index:end_index\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "start_index = torch.argmax(start_scores)\n",
    "\n",
    "end_index = torch.argmax(end_scores)\n",
    "\n",
    "answer = ' '.join(tokens[start_index:end_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corrected_answer = ''\n",
    "\n",
    "for word in answer.split():\n",
    "    \n",
    "    #If it's a subword token\n",
    "    if word[0:2] == '##':\n",
    "        corrected_answer += word[2:]\n",
    "    else:\n",
    "        corrected_answer += ' ' + word\n",
    "\n",
    "print(corrected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#重新做一個新的訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mason\\AppData\\Local\\Temp\\ipykernel_18508\\2085252174.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label']).to(device)  # 直接移動到 GPU 上\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 定義一個簡單的資料集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.texts[idx], 'label': self.labels[idx]}\n",
    "\n",
    "# 創建一個小型的訓練資料集\n",
    "texts = [\"這是一個正面的例子\", \"這是一個負面的例子\", \"這是另一個正面的例子\", \"這是另一個負面的例子\"]\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "train_dataset = CustomDataset(texts, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 初始化BertTokenizer和BertForSequenceClassification模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "\n",
    "# 定義訓練參數\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "# 訓練模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True)\n",
    "        labels = torch.tensor(batch['label']).unsqueeze(0).to(device)\n",
    "\n",
    "        inputs.to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# 保存訓練好的模型\n",
    "model.save_pretrained('your_model_directory')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用GPT-2嘗試finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '你好嗎?       11/7/2014 17:36:48 T:3486 DEBUG:'},\n",
       " {'generated_text': '你好嗎?                   None  \\n'},\n",
       " {'generated_text': '你好嗎?\\n                     '},\n",
       " {'generated_text': '你好嗎?\\n                     '},\n",
       " {'generated_text': '你好嗎? 你好嗎?             '}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='./gpt2-finetuned')\n",
    "set_seed(42)\n",
    "generator(\"你好嗎?\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"How are you\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [29:01<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1741.9966, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.233, 'train_loss': 2.3571622500865916, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned\\\\tokenizer_config.json',\n",
       " './gpt2-finetuned\\\\special_tokens_map.json',\n",
       " './gpt2-finetuned\\\\vocab.json',\n",
       " './gpt2-finetuned\\\\merges.txt',\n",
       " './gpt2-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# 載入預訓練模型和分詞器\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 載入並分詞您的自定義數據集\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"pythonTrainingData_w3schools.txt\",  # 替換為您的訓練數據集的路徑\n",
    "    block_size=128  # 根據您的數據集調整塊大小\n",
    ")\n",
    "\n",
    "# 設置訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# 初始化Trainer並fine-tuning模型\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mason\\Desktop\\作業\\科技系\\LLM\\LLM\\Fine-tuned.ipynb 儲存格 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m question_input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(question_prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# 生成答案\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m answer_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     question_input_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,  \u001b[39m# 調整生成答案的最大長度\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# 解碼並打印生成的答案\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mason/Desktop/%E4%BD%9C%E6%A5%AD/%E7%A7%91%E6%8A%80%E7%B3%BB/LLM/LLM/Fine-tuned.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, answer_ids \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(answer_output):\n",
      "File \u001b[1;32mc:\\Users\\mason\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mason\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1524\u001b[0m requires_attention_mask \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs\n\u001b[0;32m   1526\u001b[0m \u001b[39mif\u001b[39;00m model_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m requires_attention_mask \u001b[39mand\u001b[39;00m accepts_attention_mask:\n\u001b[1;32m-> 1527\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_attention_mask_for_generation(\n\u001b[0;32m   1528\u001b[0m         inputs_tensor, generation_config\u001b[39m.\u001b[39;49mpad_token_id, generation_config\u001b[39m.\u001b[39;49meos_token_id\n\u001b[0;32m   1529\u001b[0m     )\n\u001b[0;32m   1531\u001b[0m \u001b[39m# decoder-only models should use left-padding for generation\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n\u001b[0;32m   1533\u001b[0m     \u001b[39m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m     \u001b[39m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mason\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:619\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_attention_mask_for_generation\u001b[1;34m(self, inputs, pad_token_id, eos_token_id)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_attention_mask_for_generation\u001b[39m(\n\u001b[0;32m    613\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    614\u001b[0m     inputs: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m    615\u001b[0m     pad_token_id: Optional[\u001b[39mint\u001b[39m],\n\u001b[0;32m    616\u001b[0m     eos_token_id: Optional[Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]]],\n\u001b[0;32m    617\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mLongTensor:\n\u001b[0;32m    618\u001b[0m     is_input_ids \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(inputs\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m inputs\u001b[39m.\u001b[39mdtype \u001b[39min\u001b[39;00m [torch\u001b[39m.\u001b[39mint, torch\u001b[39m.\u001b[39mlong]\n\u001b[1;32m--> 619\u001b[0m     is_pad_token_in_inputs \u001b[39m=\u001b[39m (pad_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (pad_token_id \u001b[39min\u001b[39;49;00m inputs)\n\u001b[0;32m    620\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(eos_token_id, \u001b[39mint\u001b[39m):\n\u001b[0;32m    621\u001b[0m         eos_token_id \u001b[39m=\u001b[39m [eos_token_id]\n",
      "File \u001b[1;32mc:\\Users\\mason\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1068\u001b[0m, in \u001b[0;36m_EagerTensorBase.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__bool__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1068\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy())\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed\n",
    "\n",
    "# 載入預訓練模型和分詞器\n",
    "model_name = './gpt2-finetuned'  # 替換為你的微調模型的路徑\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 設定隨機種子\n",
    "set_seed(42)\n",
    "\n",
    "# 生成問題\n",
    "question_prompt = \"How can I print Hello World in Python?\"\n",
    "question_input_ids = tokenizer.encode(question_prompt, return_tensors='tf')\n",
    "\n",
    "# 生成答案\n",
    "answer_output = model.generate(\n",
    "    question_input_ids,\n",
    "    max_length=50,  # 調整生成答案的最大長度\n",
    "    num_return_sequences=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    top_k=50,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# 解碼並打印生成的答案\n",
    "for i, answer_ids in enumerate(answer_output):\n",
    "    generated_answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
    "    print(f\"Generated Answer {i+1}: {generated_answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
